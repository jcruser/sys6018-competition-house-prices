{
    "collab_server" : "",
    "contents" : "#KAGGLE HOUSE PRICES COMPETITION\n#Team = Competition 1-8\n#Elizabeth Homan, eih2nn\n#Jenn Cruser, jc4pg\n#Boh Young Suh, bs6ea\n\n##### DATA CLEANING AND PREPARATION #####\n\nlibrary(tidyverse) #Load the core tidyverse packages: ggplot2, tibble, tidyr, readr, purrr, and dplyr\n\n#Read in files:\ntrain <- read_csv(\"train.csv\") #Read in the comma separated value data file for training the model\ntest <- read_csv(\"test.csv\") #Read in the csv data file for testing the model\nsample <- read_csv(\"sample_submission.csv\") #Read in the csv data file for sample submission\n\n#Replace NA with \"None\" for all relevant columns in both training and testing sets\nNones <- c(\"Alley\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \n           \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\")\n\nfor(i in 1:length(Nones)) {\n  train[,Nones][i][is.na(train[,Nones][i])] = 'None'\n}\n\nfor(i in 1:length(Nones)) {\n  test[,Nones][i][is.na(test[,Nones][i])] = 'None'\n}\n\n#Subset to cross validate later\nsub <- sample(1:1460,size=730) \ntrain.2 <- train[sub,]     #Select subset for training\nvalidate <- train[-sub,]  #Set aside subset for validation\n\n\n#Use factor to adjust the variables that are categorical\nfactors <- c(\"MSZoning\",\"Street\",\"LotShape\",\"LandContour\",\"Utilities\",\"LotConfig\",\"LandSlope\",\n             \"Neighborhood\",\"Condition1\",\"Condition2\",\"BldgType\",\"HouseStyle\",\"RoofStyle\",\"RoofMatl\",\n             \"Exterior1st\",\"Exterior2nd\",\"MasVnrType\",\"ExterQual\",\"ExterCond\",\"Foundation\",\"BsmtQual\",\n             \"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\", \"BsmtFinType2\",\"BsmtFinSF2\",\"Heating\",\"HeatingQC\",\"CentralAir\",\n             \"Electrical\",\"KitchenQual\",\"Functional\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n             \"PavedDrive\",\"SaleType\",\"SaleCondition\",\"Alley\",\"PoolQC\", \"Fence\", \"MiscFeature\", \"FireplaceQu\")\n\ntrain.2[factors] = lapply(train.2[factors], factor)\ntest[factors] = lapply(test[factors], factor) #Also do this for the test set\nvalidate[factors] = lapply(validate[factors], factor) #Also for the validation set\n\n#Change all columns with integers to the numeric class\ntrain.2[ , (!names(train.2) %in% factors)] = lapply(train.2[ , (!names(train.2) %in% factors)], as.numeric)\ntest[ , (!names(test) %in% factors)] = lapply(test[ , (!names(test) %in% factors)], as.numeric)\nvalidate[ , (!names(validate) %in% factors)] = lapply(validate[ , (!names(validate) %in% factors)], as.numeric)\n\nsapply(train.2, class) #Check classes to make sure everything worked correctly\n\n#Create mode function\nMode <- function(x, na.rm) {\n  xtab <- table(x)\n  xmode <- names(which(xtab == max(xtab)))\n  if (length(xmode) > 1) xmode <- \">1 mode\"\n  return(xmode)\n}\n\n#Replace all NA values in \"factor\" columns with the mode of that column\nfor (var in 1:ncol(train.2)) {  \n  if (lapply((train.2[,var]), class)==\"factor\") {\n    train.2[is.na(train.2[,var]),var] <- Mode(train.2[,var], na.rm = TRUE)\n  }\n}\n\n#Repeat for test set\nfor (var in 1:ncol(test)) {  \n  if (lapply((test[,var]), class)==\"factor\") {\n    test[is.na(test[,var]),var] <- Mode(train.2[,var], na.rm = TRUE)\n  }\n}\n\n#Repeat for validation set\nfor (var in 1:ncol(validate)) {  \n  if (lapply((validate[,var]), class)==\"factor\") {\n    validate[is.na(validate[,var]),var] <- Mode(train.2[,var], na.rm = TRUE)\n  }\n}\n\n#Replace all NA values in \"numeric\" columns with the mean of that column \n#(repeat for test and validation sets)\nfor (var in 1:ncol(train.2)) {\n  if (lapply((train.2[,var]), class)==\"numeric\") {\n    train.2[is.na(train.2[,var]),var] <- sapply(train.2[,var], mean, na.rm=TRUE)\n  }\n}\n\nfor (var in 1:ncol(test)) {\n  if (lapply((test[,var]), class)==\"numeric\") {\n    test[is.na(test[,var]),var] <- sapply(train.2[,var], mean, na.rm=TRUE)\n  }\n}\n\nfor (var in 1:ncol(validate)) {\n  if (lapply((validate[,var]), class)==\"numeric\") {\n    validate[is.na(validate[,var]),var] <- sapply(train.2[,var], mean, na.rm=TRUE)\n  }\n}\n\n#Change names of columns that have numbers in them\nnames(train.2)[names(train.2) == '1stFlrSF'] <- 'FirstFlrSF'\nnames(train.2)[names(train.2) == '2ndFlrSF'] <- 'SecFlrSF'\nnames(train.2)[names(train.2) == '3SsnPorch'] <- 'TriSsnPorch'\n\nnames(test)[names(test) == '1stFlrSF'] <- 'FirstFlrSF'\nnames(test)[names(test) == '2ndFlrSF'] <- 'SecFlrSF'\nnames(test)[names(test) == '3SsnPorch'] <- 'TriSsnPorch'\n\nnames(validate)[names(validate) == '1stFlrSF'] <- 'FirstFlrSF'\nnames(validate)[names(validate) == '2ndFlrSF'] <- 'SecFlrSF'\nnames(validate)[names(validate) == '3SsnPorch'] <- 'TriSsnPorch'\n\n\n\n\n\n##### PARAMETRIC APPROACH WITH NUMERIC VARIABLES OF INTEREST TO START #####\n\n#Run a linear regression model with a few numeric variables of interest\ntrain.lm1 <- lm(SalePrice~LotArea+YearBuilt+TotalBsmtSF+FirstFlrSF+SecFlrSF+FullBath+HalfBath+TotRmsAbvGrd+PoolArea+YrSold, data=train.2)\nsummary(train.lm1)\n\n#EXAMPLE OUTPUT (FROM ONE RUN):\n#Coefficients:\n#  Estimate Std. Error t value Pr(>|t|)    \n#  (Intercept)  -2.420e+06  2.394e+06  -1.011 0.312466    \n#  LotArea       5.702e-01  1.883e-01   3.028 0.002547 ** \n#  YearBuilt     8.036e+02  7.481e+01  10.743  < 2e-16 ***\n#  TotalBsmtSF   4.678e+01  7.339e+00   6.374 3.29e-10 ***\n#  FirstFlrSF    1.001e+02  9.462e+00  10.583  < 2e-16 ***\n#  SecFlrSF      9.399e+01  7.545e+00  12.457  < 2e-16 ***\n#  FullBath     -1.205e+03  4.708e+03  -0.256 0.798127    \n#  HalfBath     -2.516e+03  4.543e+03  -0.554 0.579957    \n#  TotRmsAbvGrd -2.805e+03  1.717e+03  -1.634 0.102646    \n#  PoolArea      1.931e+02  5.400e+01   3.576 0.000372 ***\n#  YrSold        4.155e+02  1.189e+03   0.350 0.726782    \n\n#  Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\n\n#Residual standard error: 42320 on 719 degrees of freedom\n#Multiple R-squared:  0.7445,\tAdjusted R-squared:  0.741 \n#F-statistic: 209.5 on 10 and 719 DF,  p-value: < 2.2e-16\n\n#Re-run with fewer variables, based on significance from last run\ntrain.lm2 <- lm(SalePrice~LotArea+YearBuilt+TotalBsmtSF+FirstFlrSF+SecFlrSF+PoolArea, data=train.2)\nsummary(train.lm2) #All have high significance\n\nvalidpreds <- predict(train.lm2, newdata = validate)\nvalidate$predictions <- validpreds\n\nfor (i in 1:nrow(validate)) {\n  ((abs((validate[i,\"predictions\"])-(validate[i,\"SalePrice\"])))/(validate[i,\"predictions\"]))*100 -> validate[i,\"PercentOff1\"]\n}\nAvPercentOff.lm1 <- mean(validate$PercentOff1)\nAvPercentOff.lm1 #17.07737 % off of true sale price on average \n#Of note, this came down as far as 14% on other runs\n\npredict(train.lm2, newdata = test) #Use predict function to apply the linear model to the test data\nmypreds.lm <- data.frame(predict(train.lm2, newdata = test))  #Put these values into a dataframe\n\ncolnames(mypreds.lm)[1] <- \"SalePrice\"\n\nId = 1461:2919\nmypreds.lm$Id <- Id\nmypreds.lm1 <- mypreds.lm[,c(2,1)] \n\n#FIRST LM METHOD -- SCORE = 0.22 on Kaggle\nwrite.table(mypreds.lm1, file = \"eih2nn_houses_lm1.csv\", row.names=F, sep=\",\") #Write out to a csv\n\n\n\n\n\n##### ANOTHER PARAMETRIC APPROACH WITH ALL VARIABLES TO START #####\n##### SELECTED ONLY VARIABLES THAT HAD A MAJORITY OF SUBVARIABLES WITH HIGH SIGNIFICANCE  #####\n\ntrain.lm3 <- lm(SalePrice~MSSubClass+MSZoning+LotFrontage+LotArea+Street+LotShape+LandContour+\n                   LotConfig+LandSlope+Neighborhood+Condition1+Condition2+BldgType+HouseStyle+OverallQual+\n                   OverallCond+YearBuilt+YearRemodAdd+RoofStyle+RoofMatl+Exterior1st+Exterior2nd+MasVnrType+MasVnrArea+\n                   ExterQual+ExterCond+Foundation+BsmtQual+BsmtCond+BsmtExposure+BsmtFinType1+BsmtFinSF1+BsmtFinType2+\n                   BsmtFinSF2+BsmtUnfSF+TotalBsmtSF+Heating+HeatingQC+CentralAir+Electrical+FirstFlrSF+SecFlrSF+LowQualFinSF+\n                   GrLivArea+BsmtFullBath+BsmtHalfBath+FullBath+HalfBath+BedroomAbvGr+KitchenAbvGr+KitchenQual+TotRmsAbvGrd+\n                   Functional+Fireplaces+GarageType+GarageYrBlt+GarageFinish+GarageCars+GarageArea+GarageQual+GarageCond+\n                   PavedDrive+WoodDeckSF+OpenPorchSF+EnclosedPorch+TriSsnPorch+ScreenPorch+PoolArea+MiscVal+MoSold+YrSold+\n                   SaleType+SaleCondition, data=train.2)\n\nsummary(train.lm3) #See selected variables below for which ones consistently had a significance code above 0.001 \n\ntrain.lm4 <- lm(SalePrice~LotArea+OverallQual+OverallCond+YearBuilt+RoofMatl+\n                  ExterQual+BsmtFinSF1, data=train.2)\nsummary(train.lm4)\n\ntrain.lm5 <- lm(SalePrice~LotArea+OverallQual+ExterQual+BsmtFinSF1, data=train.2)\nsummary(train.lm5)\nmse.lm5 <- mean(train.lm5$residuals^2)\nmse.lm5 #1860451135...\n\nvalidpreds2 <- predict(train.lm5, newdata = validate)\nvalidate$predictions2 <- validpreds2\n\nfor (i in 1:nrow(validate)) {\n  ((abs((validate[i,\"predictions2\"])-(validate[i,\"SalePrice\"])))/(validate[i,\"predictions2\"]))*100 -> validate[i,\"PercentOff2\"]\n}\nAvPercentOff.lm2 <- mean(validate$PercentOff2)\nAvPercentOff.lm2 #16.89892 % off of true sale price on average \n#This got as low as 16.3% on other runs, but was generally comparable to the \n#results from the other linear model approach\n\n#WRITE UP TEST PREDITIONS\npredict(train.lm5, newdata = test) #Use predict function to apply the linear model to the test data\nmypreds.lm2 <- data.frame(predict(train.lm5, newdata = test))  #Put these values into a dataframe\n\ncolnames(mypreds.lm2)[1] <- \"SalePrice\"\n\nId = 1461:2919\nmypreds.lm2$Id <- Id\nmypreds.lm3 <- mypreds.lm2[,c(2,1)] \n\nmypreds.lm3[757,2] <- 0\n\n#SECOND LM METHOD -- SCORE = 0.25 on Kaggle (worse score, as expected)\nwrite.table(mypreds.lm3, file = \"eih2nn_houses_lm2.csv\", row.names=F, sep=\",\") #Write out to a csv\n\n\n\n##### ANOTHER PARAMETRIC -- ABOVE BUT INCLUDING ALL VARIABLES WITH ANY SUBVARIABLES OF HIGH SIGNIFICANCE (**)  #####\n\ntrain.lm6 <- lm(SalePrice~LotArea+Neighborhood+OverallQual+OverallCond+\n                   YearBuilt+RoofMatl+ExterQual+BsmtQual+BsmtExposure+\n                   BsmtUnfSF+FirstFlrSF+SecFlrSF+KitchenQual+PoolArea+ScreenPorch, data=train.2)\n\nsummary(train.lm6)\n\n#Of note, we were unable to use validation due to variables that are not consistent across both\n#the training and validations sets (i.e. subvariables with too few observations across the whole set)\n\nmypreds.lm3 <- data.frame(predict(train.lm6, newdata = test))  #Put these values into a vector\ncolnames(mypreds.lm3)[1] <- \"SalePrice\"\nId = 1461:2919\nmypreds.lm3$Id <- Id\nmypreds.lm3 <- mypreds.lm3[,c(2,1)] \n\n\n#THIRD LM METHOD -- SCORE = 0.158 on Kaggle (best score)\nwrite.table(mypreds.lm3, file = \"eih2nn_houses_lm3.csv\", row.names=F, sep=\",\") #Write out to a csv\n\n######################################################################\n\n##### NON-PARAMETRIC APPROACH #####\n\n#Install and load class and caret packages to run knn function \ninstall.packages(\"class\")\nlibrary(class)  \ninstall.packages(\"caret\")\nlibrary(caret)\n\ntrain.3 <- train.2[ , (!names(train.2) %in% factors)] #Select only columns with numeric values\ntest.2 <- test[ , (!names(test) %in% factors)] #Repeat for test set\nvalidate.2 <- validate[ , (!names(validate) %in% factors)] #Repeat for validation set\n\ntrctrl <- trainControl(method = \"repeatedcv\", number = 10, repeats = 1)\nset.seed(22)\nknn.fit <- train(SalePrice ~., data = train.3, method = \"knn\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10)\nknn.fit\n#k-Nearest Neighbors \n#730 samples\n#36 predictor\n\n#Pre-processing: centered (36), scaled (36) \n#Resampling: Cross-Validated (10 fold, repeated 1 times) \n#Summary of sample sizes: 658, 658, 657, 656, 656, 658, ... \n#Resampling results across tuning parameters:\n\n#k   RMSE      Rsquared \n#5  40701.88  0.7586845\n#7  40595.15  0.7667168\n#9  39956.75  0.7763865\n#11  40162.69  0.7801505\n#13  40197.83  0.7841858\n#15  40091.22  0.7894252\n#17  40388.08  0.7894150\n#19  40211.36  0.7933000\n#21  40339.65  0.7947003\n#23  40536.55  0.7950865\n\n#RMSE was used to select the optimal model using  the smallest value.\n#The final value used for the model was k = 9.\n\nvalidate.3 <- subset(validate.2, select = -c(predictions, predictions2, PercentOff1, PercentOff2)) #Create new df without those columns\nvalidate.4 <- subset(validate.2, select = -c(SalePrice))\n\nknn.validate <- predict(knn.fit, newdata = validate.4)\nvalidate.3$knnpredictions <- knn.validate\n\nfor (i in 1:nrow(validate.3)) {\n  ((abs((validate.3[i,\"knnpredictions\"])-(validate.3[i,\"SalePrice\"])))/(validate.3[i,\"SalePrice\"]))*100 -> validate.3[i,\"PercentOff\"]\n}\nAvPercentOffKNN <- mean(validate.3$PercentOff)\nAvPercentOffKNN #12.77642 % off of true sale price on average\n\nknn.preds <- predict(knn.fit, newdata = test.2)\nknn.preds <- data.frame(predict(knn.fit, newdata = test.2))\n\ncolnames(knn.preds)[1] <- \"SalePrice\"\nknn.preds$Id <- Id\nknn.preds1 <- knn.preds[,c(2,1)]\n\n#KNN PREDICTIONS -- SCORE = 0.194 on Kaggle (second best score)\nwrite.table(knn.preds1, file = \"eih2nn_houses_knn.csv\", row.names=F, sep=\",\") #Write out to a csv\n\n\n##### NON-PARAMETRIC APPROACH WITH KNN AND K=13 #####\n\nset.seed(150)\nknn.fit2 <- train(SalePrice ~., data = train.3, method = \"knn\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneLength = 10)\nknn.fit2\n#K = 13\n\nvalidate.5 <- subset(validate.2, select = -c(predictions, predictions2, PercentOff1, PercentOff2)) #Create new df without those columns\nvalidate.6 <- subset(validate.2, select = -c(SalePrice))\n\nknn.validate2 <- predict(knn.fit2, newdata = validate.6)\nvalidate.5$knnpredictions2 <- knn.validate2\n\nfor (i in 1:nrow(validate.5)) {\n  ((abs((validate.5[i,\"knnpredictions2\"])-(validate.5[i,\"SalePrice\"])))/(validate.5[i,\"SalePrice\"]))*100 -> validate.5[i,\"PercentOff\"]\n}\nAvPercentOffKNN2 <- mean(validate.5$PercentOff)\nAvPercentOffKNN2 #12.34 % off of true sale price on average\n\nknn.preds2 <- data.frame(predict(knn.fit2, newdata = test.2))\n\ncolnames(knn.preds2)[1] <- \"SalePrice\"\nknn.preds2$Id <- Id\nknn.preds3 <- knn.preds2[,c(2,1)]\n\n#KNN PREDICTIONS - SCORE = 0.195 on kaggle (not as good as when k=9)\nwrite.table(knn.preds3, file = \"eih2nn_houses_knn2.csv\", row.names=F, sep=\",\") #Write out to a csv\n\n\n",
    "created" : 1504543298282.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2560783817",
    "id" : "739E06E0",
    "lastKnownWriteTime" : 1504570731,
    "last_content_update" : 1504571416852,
    "path" : "~/Documents/GitHub/house-prices/sys6018-competition-house-prices/team_competition_1_8.R",
    "project_path" : "team_competition_1_8.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}